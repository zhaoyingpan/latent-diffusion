{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "# Class-Conditional Synthesis with Latent Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh7u8gOx0ivw"
      },
      "source": [
        "Install all the requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/panzy/latent-diffusion\n"
          ]
        }
      ],
      "source": [
        "%cd /home/panzy/latent-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(\"/n/owens-data1/mnt/big2/data/panzy/latent/imgnet/cin256-v2.yaml\")  \n",
        "    model = load_model_from_config(config, \"/n/owens-data1/mnt/big2/data/panzy/latent/imgnet/model.ckpt\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnyd-XUKbfE",
        "outputId": "72494988-1623-4504-cd84-4ff065ad1753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from /n/owens-data1/mnt/big2/data/panzy/latent/imgnet/model.ckpt\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 400.92 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n"
          ]
        }
      ],
      "source": [
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "model = get_model()\n",
        "sampler = DDIMSampler(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEAhY8AhUrh"
      },
      "source": [
        "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np \n",
        "# from PIL import Image\n",
        "# from einops import rearrange\n",
        "# from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "# classes = [847, 895]   # define classes to be sampled here\n",
        "# n_samples_per_class = 10\n",
        "\n",
        "# ddim_steps = 20\n",
        "# ddim_eta = 0.0\n",
        "# scale = 3.0   # for unconditional guidance\n",
        "\n",
        "\n",
        "# all_samples = list()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     with model.ema_scope():\n",
        "#         uc = model.get_learned_conditioning(\n",
        "#             {model.cond_stage_key: torch.tensor(n_samples_per_class*[1000]).to(model.device)}\n",
        "#             )\n",
        "        \n",
        "#         for class_label in classes:\n",
        "#             print(f\"rendering {n_samples_per_class} examples of class '{class_label}' in {ddim_steps} steps and using s={scale:.2f}.\")\n",
        "#             xc = torch.tensor(n_samples_per_class*[class_label])\n",
        "#             c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
        "            \n",
        "#             samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "#                                              conditioning=c,\n",
        "#                                              batch_size=n_samples_per_class,\n",
        "#                                              shape=[3, 64, 64],\n",
        "#                                              verbose=False,\n",
        "#                                              unconditional_guidance_scale=scale,\n",
        "#                                              unconditional_conditioning=uc, \n",
        "#                                              eta=ddim_eta)\n",
        "\n",
        "#             x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "#             x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, \n",
        "#                                          min=0.0, max=1.0)\n",
        "#             all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "# # display as grid\n",
        "# grid = torch.stack(all_samples, 0)\n",
        "# grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "# grid = make_grid(grid, nrow=n_samples_per_class)\n",
        "\n",
        "# # to image\n",
        "# grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "# Image.fromarray(grid.astype(np.uint8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir /home/panzy/latent-diffusion/outputs/full_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "jcbqWX2Ytu9t",
        "outputId": "396e86dd-85df-4dda-bc0e-651e63cc8912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape for DDIM sampling is (10, 3, 64, 64), eta 0.0\n",
            "Running DDIM Sampling with 20 timesteps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DDIM Sampler: 100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape for DDIM sampling is (10, 3, 64, 64), eta 0.0\n",
            "Running DDIM Sampling with 20 timesteps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DDIM Sampler: 100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "img_root = '/n/owens-data1/mnt/big2/data/panzy/diffusion_fake_file/latent_diffusion'\n",
        "exp_name = 'full_dataset'\n",
        "classes = [847, 895]   # define classes to be sampled here\n",
        "n_samples_per_class = 2500\n",
        "batch_size = 10\n",
        "num_iter = np.ceil(n_samples_per_class / batch_size).astype(np.int32) - 141\n",
        "num_iter = 1\n",
        "\n",
        "ddim_steps = 20\n",
        "ddim_eta = 0.0\n",
        "scale = 3.0   # for unconditional guidance\n",
        "\n",
        "\n",
        "all_samples = list()\n",
        "\n",
        "with torch.no_grad():\n",
        "    with model.ema_scope():\n",
        "        uc = model.get_learned_conditioning(\n",
        "            {model.cond_stage_key: torch.tensor(batch_size*[1000]).to(model.device)}\n",
        "            )\n",
        "        for iter in range(num_iter):\n",
        "            samples = []\n",
        "            for class_label in classes:\n",
        "                xc = torch.tensor(batch_size*[class_label])\n",
        "                c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
        "                \n",
        "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                                conditioning=c,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shape=[3, 64, 64],\n",
        "                                                verbose=False,\n",
        "                                                unconditional_guidance_scale=scale,\n",
        "                                                unconditional_conditioning=uc, \n",
        "                                                eta=ddim_eta)\n",
        "\n",
        "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, \n",
        "                                            min=0.0, max=1.0)\n",
        "                samples.append(x_samples_ddim)\n",
        "            \n",
        "            samples = torch.stack(samples)\n",
        "            samples = samples.reshape(-1, samples.shape[-3], samples.shape[-2], samples.shape[-1])\n",
        "            filename = os.path.join(img_root, exp_name, 'iter_140.pt')\n",
        "            torch.save(samples, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_images = torch.stack(all_samples)\n",
        "all_images = all_images.reshape(-1, all_images.shape[-3], all_images.shape[-2], all_images.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "92QkRfm0e6K0"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# for i in range(all_images.shape[0]):\n",
        "#   image = all_images[i].squeeze().flip([0])\n",
        "#   cv2.imwrite('latent/'+str(i).zfill(2)+'.jpg', 255*image.permute(1,2,0).cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agDzTJR9Pa2k",
        "outputId": "ecfee8f8-ee85-402b-812c-24894e8568e2"
      },
      "outputs": [],
      "source": [
        "torch.save(all_images, 'img.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of latent-imagenet-diffusion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('ldm': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ed6b7d6d91d06d69890aa42585819f00f8861835874d51525633853c66ef8bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
